export async function onRequestPost(context) {
  const { request, env } = context;

  const { content } = await request.json();

  if (!content || content.length < 50) {
    return new Response(
      JSON.stringify({ error: "Content too short" }),
      { status: 400 }
    );
  }

  try {
    const result = await summarizeWithGroq(content, env);
    return json(result);
  } catch (e1) {
    try {
      const result = await summarizeWithGemini(content, env);
      return json(result);
    } catch (e2) {
      try {
        const result = await summarizeWithHF(content, env);
        return json(result);
      } catch (e3) {
        return json("All providers failed", 500);
      }
    }
  }
}

function json(data, status = 200) {
  return new Response(JSON.stringify({ summary: data }), {
    status,
    headers: { "Content-Type": "application/json" },
  });
}

const SYSTEM_PROMPT = `
Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…Ø­ØªÙˆÙ‰ ØªÙ‚Ù†ÙŠ ÙˆÙ…Ø­Ø±Ø± SEO Ù…Ø­ØªØ±Ù. Ù…Ù‡Ù…ØªÙƒ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ Ù‡ÙŠÙƒÙ„ÙŠØ© Ù…Ø­Ø¯Ø¯Ø© Ø¬Ø¯Ø§Ù‹ ÙˆØ¬Ø°Ø§Ø¨Ø©.
Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
1. Ø¹Ù†ÙˆØ§Ù† ÙØ±Ø¹ÙŠ Ø¬Ø°Ø§Ø¨ Ù„Ù„Ù…Ù„Ø®Øµ (Ù…Ø«Ù„Ø§Ù‹: ðŸ“Œ Ø§Ù„Ø²Ø¨Ø¯Ø© Ø§Ù„ØªÙ‚Ù†ÙŠØ©).
2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ 5 Ù†Ù‚Ø§Ø· Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ø±ÙƒØ²Ø© ÙˆÙˆØ§Ø¶Ø­Ø© Ø¬Ø¯Ø§Ù‹ (Bullet points).
3. Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù…Ø¨Ø§Ø´Ø±Ø§Ù‹ØŒ Ø®Ø§Ù„ÙŠØ§Ù‹ Ù…Ù† Ø§Ù„Ø­Ø´ÙˆØŒ ÙˆÙ…Ù†Ø§Ø³Ø¨Ø§Ù‹ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø© (Skimming).
4. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø¯Ù‚Ø© (Markdown):

### ðŸš€ Ø§Ù„Ø®Ù„Ø§ØµØ© ÙÙŠ Ù†Ù‚Ø§Ø·
* **Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„Ù‚ÙˆÙŠØ©:** Ø´Ø±Ø­ Ù…Ø®ØªØµØ±.
* **Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©:** Ø´Ø±Ø­ Ù…Ø®ØªØµØ±.
* **Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø©:** Ø´Ø±Ø­ Ù…Ø®ØªØµØ±.
* **Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø©:** Ø´Ø±Ø­ Ù…Ø®ØªØµØ±.
* **Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø®Ø§Ù…Ø³Ø©:** Ø´Ø±Ø­ Ù…Ø®ØªØµØ±.

Ù„Ø§ ØªØ¶Ù Ø£ÙŠ Ù…Ù‚Ø¯Ù…Ø§Øª Ø£Ùˆ Ø®Ø§ØªÙ…Ø§Øª Ø®Ø§Ø±Ø¬ Ù‡Ø°Ø§ Ø§Ù„Ù‡ÙŠÙƒÙ„.
`;

async function summarizeWithGroq(content, env) {
  const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
    method: "POST",
    headers: {
      "Authorization": `Bearer ${env.GROQ_API_KEY}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model: "llama-3.1-8b-instant",
      messages: [
        { role: "system", content: SYSTEM_PROMPT },
        { role: "user", content }
      ],
      temperature: 0.3
    }),
  });

  if (!response.ok) throw new Error("Groq failed");
  const data = await response.json();
  return data.choices[0].message.content;
}

async function summarizeWithGemini(content, env) {
  const response = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=${env.GEMINI_API_KEY}`,
    {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        contents: [
          {
            parts: [
              { text: SYSTEM_PROMPT },
              { text: content }
            ]
          }
        ]
      }),
    }
  );

  if (!response.ok) throw new Error("Gemini failed");
  const data = await response.json();
  return data.candidates[0].content.parts[0].text;
}

async function summarizeWithHF(content, env) {
  // HuggingFace models usually take plain text, prompt engineering is harder here but we try.
  const response = await fetch(
    "https://api-inference.huggingface.co/models/facebook/bart-large-cnn",
    {
      method: "POST",
      headers: {
        "Authorization": `Bearer ${env.HF_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({ inputs: "Summarize this in 5 bullet points in Arabic: " + content }),
    }
  );

  if (!response.ok) throw new Error("HF failed");
  const data = await response.json();
  return data[0].summary_text;
}